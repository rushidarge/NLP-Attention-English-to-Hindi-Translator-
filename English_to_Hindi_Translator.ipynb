{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English to Hindi Translator",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data download"
      ],
      "metadata": {
        "id": "egbcn9Z3aZ9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzZnBMaioNLu",
        "outputId": "97c87811-a31a-4d6d-9eec-66680c791c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kaggle.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile kaggle.json\n",
        "{\"username\":\"rushikeshdarge\",\"key\":\"********************************\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d vaibhavkumar11/hindi-english-parallel-corpus\n",
        "!unzip /content/hindi-english-parallel-corpus.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFmCsKC8pLyP",
        "outputId": "93de6793-e395-45bc-ccc0-cd63d7bc9a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Downloading hindi-english-parallel-corpus.zip to /content\n",
            " 94% 105M/112M [00:01<00:00, 66.5MB/s] \n",
            "100% 112M/112M [00:01<00:00, 66.0MB/s]\n",
            "Archive:  /content/hindi-english-parallel-corpus.zip\n",
            "  inflating: hindi_english_parallel.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "RiCi4Rcipd-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download model to check the language from fasttext"
      ],
      "metadata": {
        "id": "Kr_z8VNfaebe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1aUDcugDOSp",
        "outputId": "a4623d18-e5da-4e42-90ce-89dd595119ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3138162 sha256=a68b0bb55a2c33cc1e37e2a850c03411c334d62fdf1c874fa4a611f9e33fafe0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://fasttext.cc/docs/en/language-identification.html\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTS5uXhHOzAA",
        "outputId": "ac0c445c-a0b7-4896-bcac-ce63e5a2edca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-09 05:28:53--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 938013 (916K) [binary/octet-stream]\n",
            "Saving to: ‘lid.176.ftz’\n",
            "\n",
            "lid.176.ftz         100%[===================>] 916.03K  1.19MB/s    in 0.8s    \n",
            "\n",
            "2022-07-09 05:28:55 (1.19 MB/s) - ‘lid.176.ftz’ saved [938013/938013]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "model = fasttext.load_model('/content/lid.176.ftz')\n",
        "print(model.predict('निचले पटल के लिए डिफोल्ट प्लग-इन खाका', k=1))  # top 2 matching languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb1dcb0-0728-4f67-861f-f03f675c26a6",
        "id": "eoTy0aEUDOSs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(('__label__hi',), array([0.95343572]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset"
      ],
      "metadata": {
        "id": "bk17rVccalQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/hindi_english_parallel.csv', nrows=50000)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cyp8a7sXptQr",
        "outputId": "bb0f3484-7d36-4c14-a2f0-8dc900a896ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               hindi  \\\n",
              "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
              "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
              "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
              "\n",
              "                                          english  \n",
              "0  Give your application an accessibility workout  \n",
              "1               Accerciser Accessibility Explorer  \n",
              "2  The default plugin layout for the bottom panel  \n",
              "3     The default plugin layout for the top panel  \n",
              "4  A list of plugins that are disabled by default  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91e1fdc0-aad9-4035-ac5d-1422e1aff7ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
              "      <td>Give your application an accessibility workout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
              "      <td>Accerciser Accessibility Explorer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the bottom panel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the top panel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
              "      <td>A list of plugins that are disabled by default</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91e1fdc0-aad9-4035-ac5d-1422e1aff7ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91e1fdc0-aad9-4035-ac5d-1422e1aff7ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91e1fdc0-aad9-4035-ac5d-1422e1aff7ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw-wVw8CrjI-",
        "outputId": "659b3e0d-d87c-46ef-d8a3-5b8ffadc9886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hindi      3\n",
              "english    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "bWeF1I3rsCPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RloOL7IesCMN",
        "outputId": "e088f436-c12d-4527-85ab-4287830930e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49997, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "UiiL7rP5sCBv",
        "outputId": "451358b6-1860-4397-b4da-530a4fb8a764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 hindi                     english\n",
              "10809     बिल्ड आटोटूल             Build Autotools\n",
              "38816  चयनित मौन हटाएँ  Remove the selected slices\n",
              "48845        खराब घटना                   Bad event\n",
              "39287         कुंजीपटल                    Keyboard"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90d6bbb2-8541-4f4f-b74e-586eb550c961\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10809</th>\n",
              "      <td>बिल्ड आटोटूल</td>\n",
              "      <td>Build Autotools</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38816</th>\n",
              "      <td>चयनित मौन हटाएँ</td>\n",
              "      <td>Remove the selected slices</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48845</th>\n",
              "      <td>खराब घटना</td>\n",
              "      <td>Bad event</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39287</th>\n",
              "      <td>कुंजीपटल</td>\n",
              "      <td>Keyboard</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90d6bbb2-8541-4f4f-b74e-586eb550c961')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-90d6bbb2-8541-4f4f-b74e-586eb550c961 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-90d6bbb2-8541-4f4f-b74e-586eb550c961');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from string import digits"
      ],
      "metadata": {
        "id": "AyjYCONh8q1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning dataset"
      ],
      "metadata": {
        "id": "e-5lLTaoaodR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase all characters\n",
        "df['english']=df['english'].apply(lambda x: x.lower())\n",
        "# Remove quotes\n",
        "df['english']=df['english'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "df['hindi']=df['hindi'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "# Remove all the special characters\n",
        "df['english']=df['english'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "df['hindi']=df['hindi'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "# Remove all numbers from text\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "df['english']=df['english'].apply(lambda x: x.translate(remove_digits))\n",
        "df['hindi']=df['hindi'].apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "df['hindi'] = df['hindi'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "# if english text in hindi column\n",
        "df['hindi'] = df['hindi'].apply(lambda x: re.sub(\"[A-Za-z]\", \"\", x))\n",
        "\n",
        "# remove extra\n",
        "df['english']=df['english'].apply(lambda x: re.sub('[-_.:;\\[\\]\\|,]', '', x))\n",
        "df['hindi']=df['hindi'].apply(lambda x: re.sub('[-_.;\\[\\]\\|,]', '', x))\n",
        "\n",
        "# Remove extra spaces\n",
        "df['english']=df['english'].apply(lambda x: x.strip())\n",
        "df['hindi']=df['hindi'].apply(lambda x: x.strip())\n",
        "df['english']=df['english'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "df['hindi']=df['hindi'].apply(lambda x: re.sub(\" +\", \" \", x))"
      ],
      "metadata": {
        "id": "402oxwc29HSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r7uCzzlS9HO_",
        "outputId": "8e68d8fb-7e41-4b86-cd2c-fecee667a135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   hindi  \\\n",
              "49092  अधिकतम वीडियो बिटरेट में वीडियो गुणवत्ता और प्...   \n",
              "33155                      फ़ाइल पूर्वावलोकन सक्रिय करें   \n",
              "41435  सामग्री को दोहराने के बजाय उन्हें क्षैतिज स्के...   \n",
              "46386                                  दो बटन गतिशील मोड   \n",
              "19448                                           कमिट कोः   \n",
              "\n",
              "                                                 english  \n",
              "49092  the maximum video bitrate in kbits s the video...  \n",
              "33155                                enable file preview  \n",
              "41435  repeat the contents rather than scaling them h...  \n",
              "46386                            two button dynamic mode  \n",
              "19448                              commit to cherry pick  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-974f8ee1-5605-4992-b44a-496fd0501e08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49092</th>\n",
              "      <td>अधिकतम वीडियो बिटरेट में वीडियो गुणवत्ता और प्...</td>\n",
              "      <td>the maximum video bitrate in kbits s the video...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33155</th>\n",
              "      <td>फ़ाइल पूर्वावलोकन सक्रिय करें</td>\n",
              "      <td>enable file preview</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41435</th>\n",
              "      <td>सामग्री को दोहराने के बजाय उन्हें क्षैतिज स्के...</td>\n",
              "      <td>repeat the contents rather than scaling them h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46386</th>\n",
              "      <td>दो बटन गतिशील मोड</td>\n",
              "      <td>two button dynamic mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19448</th>\n",
              "      <td>कमिट कोः</td>\n",
              "      <td>commit to cherry pick</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-974f8ee1-5605-4992-b44a-496fd0501e08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-974f8ee1-5605-4992-b44a-496fd0501e08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-974f8ee1-5605-4992-b44a-496fd0501e08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_hindi'] = True\n",
        "df['is_english'] = True"
      ],
      "metadata": {
        "id": "xM-TyQBQtTyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check language of text"
      ],
      "metadata": {
        "id": "wn4rTnN3ayN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error = []\n",
        "count = 0\n",
        "for i,row in tqdm(df.iterrows()):\n",
        "    hi_tex = df.iloc[i]['hindi']\n",
        "    en_tex = df.iloc[i]['english']\n",
        "    try:\n",
        "        hin_pred = model.predict(hi_tex,k=3)[0]\n",
        "        if set(['__label__hi']).issubset(hin_pred) or set(['__label__mr']).issubset(hin_pred):\n",
        "            pass\n",
        "        else:\n",
        "            df.at[i,'is_hindi'] = False\n",
        "            count += 1\n",
        "        en_pred = model.predict(en_tex,k=3)[0]\n",
        "        if set(['__label__en']).issubset(en_pred):\n",
        "            pass\n",
        "        else:\n",
        "            df.at[i,'is_english'] = False\n",
        "            count += 1\n",
        "    except:\n",
        "        error.append(i)"
      ],
      "metadata": {
        "id": "v6DcemYks9cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.is_english == False].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx7uOKqxAi_5",
        "outputId": "51e1e3fa-d750-4b24-940e-176bd33c4ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2454, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.is_hindi == False].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erx9UF_UAn1S",
        "outputId": "2646a180-13f9-4af7-9581-6671b0f7ee64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2780, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmVSPp9v_oeo",
        "outputId": "e0ac1ab9-d1f0-4398-a00d-a322546fa4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49997, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df.is_hindi == False]\n",
        "df = df[~df.is_english == False]\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44fSLirjy769",
        "outputId": "93d5bf44-977e-4715-9574-4db747f26cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45032, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['is_hindi','is_english'],inplace=True)"
      ],
      "metadata": {
        "id": "vZUzaM04De75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Len of sentence"
      ],
      "metadata": {
        "id": "1PuVr2emE0Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['hindi_len'] = df['hindi'].apply(lambda x: len(x.split()))\n",
        "df['english_len'] = df['english'].apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "J7kf5J3lEkHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df.hindi_len, bins=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "Wt58A0bqEkFM",
        "outputId": "54852557-8f34-4de0-e594-95f1548c945e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1f4dba310>"
            ]
          },
          "metadata": {},
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df.english_len, bins=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zO_XhcMCEkCP",
        "outputId": "2c50efac-0c6f-41b2-f614-e95b966b73d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd1f4e30ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english\n",
        "for i in np.arange(0.1,1.1,0.1):\n",
        "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(df.english_len, i)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD9rfn45FGsw",
        "outputId": "7db7a7a4-8b12-4bf3-fc5a-cb1e04ec86a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Quantile is 1.0\n",
            "20 Quantile is 2.0\n",
            "30 Quantile is 2.0\n",
            "40 Quantile is 2.0\n",
            "50 Quantile is 3.0\n",
            "60 Quantile is 4.0\n",
            "70 Quantile is 5.0\n",
            "80 Quantile is 6.0\n",
            "90 Quantile is 9.0\n",
            "100 Quantile is 111.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english\n",
        "for i in np.arange(0.9,1.01,0.01):\n",
        "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(df.english_len, i)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ5S-j2sFGqj",
        "outputId": "70d6ab10-d709-4165-e452-816c0ca57e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90 Quantile is 9.0\n",
            "91 Quantile is 9.0\n",
            "92 Quantile is 10.0\n",
            "93 Quantile is 10.0\n",
            "94 Quantile is 11.0\n",
            "95 Quantile is 11.0\n",
            "96 Quantile is 12.0\n",
            "97 Quantile is 14.0\n",
            "98 Quantile is 17.0\n",
            "99 Quantile is 24.0\n",
            "100 Quantile is 111.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hindi\n",
        "for i in np.arange(0.1,1.1,0.1):\n",
        "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(df.hindi_len, i)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaL2QvImG1Uw",
        "outputId": "2facfb3d-fa70-4015-c571-6f40b9bcfc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Quantile is 1.0\n",
            "20 Quantile is 1.0\n",
            "30 Quantile is 2.0\n",
            "40 Quantile is 2.0\n",
            "50 Quantile is 3.0\n",
            "60 Quantile is 3.0\n",
            "70 Quantile is 4.0\n",
            "80 Quantile is 6.0\n",
            "90 Quantile is 8.0\n",
            "100 Quantile is 110.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hindi\n",
        "for i in np.arange(0.9,1.01,0.01):\n",
        "    print('{0} Quantile is {1}'.format(int(i*100),np.quantile(df.hindi_len, i)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elZFJF4OFGoU",
        "outputId": "d13838b5-4078-4ae2-e7c9-39dfe22ec4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90 Quantile is 8.0\n",
            "91 Quantile is 9.0\n",
            "92 Quantile is 9.0\n",
            "93 Quantile is 10.0\n",
            "94 Quantile is 11.0\n",
            "95 Quantile is 11.0\n",
            "96 Quantile is 12.0\n",
            "97 Quantile is 14.0\n",
            "98 Quantile is 16.0\n",
            "99 Quantile is 24.0\n",
            "100 Quantile is 110.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For both we only those data who has less than or euqal to 10"
      ],
      "metadata": {
        "id": "JGXOwBbuHAvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.english_len<=10]\n",
        "df = df[df.hindi_len<=10]\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bilAgA5yFGlm",
        "outputId": "a356e07f-937d-4a6d-d987-7217c6e3d72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41424, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['hi_inp'] = '<start> ' + df.hindi \n",
        "df['hi_out'] = df.hindi + ' <end>' "
      ],
      "metadata": {
        "id": "XMZEP37OIfIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['english_len','hindi_len', 'hindi'],inplace=True)"
      ],
      "metadata": {
        "id": "nOmA_P3CIHjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, validation = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "diLUzMOXIGZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking shape\n",
        "print(train.shape, validation.shape)\n",
        "\n",
        "# add <end> in first row so we can use same tokenizer for both hi_inp and eng_out\n",
        "train.hi_inp.iloc[0]= str(train.hi_inp.iloc[0]) + ' <end>'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rXyCQQVFGjQ",
        "outputId": "53b7c2e9-60b6-47bb-e9cc-66c2c008e855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33139, 3) (8285, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "OHaAXAJna3Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "GM0_vq0SFGgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization on data\n",
        "token_eng = Tokenizer()\n",
        "token_eng.fit_on_texts(train['english'].values)\n",
        "token_hi = Tokenizer(filters='')\n",
        "token_hi.fit_on_texts(train['hi_inp'].values)"
      ],
      "metadata": {
        "id": "vTFuriOIHzWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size_eng=len(token_eng.word_index.keys())\n",
        "print('Vocab size of english is',vocab_size_eng)\n",
        "vocab_size_hi=len(token_hi.word_index.keys())\n",
        "print('Vocab size of hindi is',vocab_size_hi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZgnTxMNHzTh",
        "outputId": "7a5601a7-574d-40da-fff5-9d435fb3ac92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size of english is 3002\n",
            "Vocab size of hindi is 3217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_hi.word_index['<start>'], token_hi.word_index['<end>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzlVWxtkHzRA",
        "outputId": "d7a6f1de-86e1-43e2-82d0-60a6667a02e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2867)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to numbers\n",
        "train_dec_in = token_hi.texts_to_sequences(train.hi_inp)\n",
        "train_hi_inp = token_eng.texts_to_sequences(train.english)"
      ],
      "metadata": {
        "id": "2H8sKo43HzO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10\n",
        "# test to sequence\n",
        "train_dec_in = token_hi.texts_to_sequences(train.hi_inp)\n",
        "train_dec_out = token_hi.texts_to_sequences(train.hi_out)\n",
        "train_eng_inp = token_eng.texts_to_sequences(train.english)\n",
        "\n",
        "# padding\n",
        "train_dec_in_seq = pad_sequences(train_dec_in, maxlen=max_len, padding='post', dtype='int32')\n",
        "train_dec_out_seq = pad_sequences(train_dec_out, maxlen=max_len, padding='post', dtype='int32')\n",
        "train_eng_inp_seq = pad_sequences(train_eng_inp, maxlen=max_len, padding='post', dtype='int32')\n",
        "\n",
        "# test to sequence\n",
        "test_dec_in = token_hi.texts_to_sequences(validation.hi_inp)\n",
        "test_dec_out = token_hi.texts_to_sequences(validation.hi_out)\n",
        "test_eng_inp = token_eng.texts_to_sequences(validation.english)\n",
        "\n",
        "# padding\n",
        "test_dec_in_seq = pad_sequences(test_dec_in, maxlen=max_len, padding='post', dtype='int32')\n",
        "test_dec_out_seq = pad_sequences(test_dec_out, maxlen=max_len, padding='post', dtype='int32')\n",
        "test_eng_inp_seq = pad_sequences(test_eng_inp, maxlen=max_len, padding='post', dtype='int32')"
      ],
      "metadata": {
        "id": "SCX5napvLBUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_input_length, dec_input_length, dec_out_length = 10,10,10"
      ],
      "metadata": {
        "id": "ZokjgB8bLZQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "print('Encoder text :',train.english.iloc[index])\n",
        "print('Representation :',train_eng_inp_seq[index])\n",
        "\n",
        "print('hi text :', train.hi_inp.iloc[index])\n",
        "print('Decoder input :',train_dec_in_seq[index])\n",
        "\n",
        "print('Decoder output :',train.hi_out.iloc[index])\n",
        "print('Decoder output :',train_dec_out_seq[index])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlNlodTRLmhy",
        "outputId": "fd722b7c-79ea-4648-d01b-c30687e86276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder text : forward\n",
            "Representation : [515   0   0   0   0   0   0   0   0   0]\n",
            "hi text : <start> आगे <end>\n",
            "Decoder input : [   1  486 2867    0    0    0    0    0    0    0]\n",
            "Decoder output : आगे <end>\n",
            "Decoder output : [ 486 2867    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionaries from num to word and vice versa\n",
        "hi_index_word = {}\n",
        "hi_word_index = {}\n",
        "\n",
        "for key, value in token_hi.word_index.items():\n",
        "    hi_index_word[value] = key\n",
        "    hi_word_index[key] = value"
      ],
      "metadata": {
        "id": "PicMY9qpMDzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hi_vocab_size = len(token_hi.word_index)+1\n",
        "eng_vocab_size = len(token_eng.word_index)+1\n",
        "\n",
        "print('hindi vocab size',hi_vocab_size)\n",
        "print('english vocab size',eng_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNCkpFgxLt9b",
        "outputId": "f662bb81-9e9c-4ab1-a1fc-8a0bf0bb1e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hindi vocab size 3218\n",
            "english vocab size 3003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generator"
      ],
      "metadata": {
        "id": "evoiS36vN81K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
        "\n",
        "def data_generator(encoder_inp, decoder_inp, decoder_out, batch_size):\n",
        "    '''\n",
        "    Data Generator for model training\n",
        "    It returns list of encoder_imput and decoder_input of each shape [batch_size, max_len]\n",
        "    and decoder_out of shape (batch_size, max_len)\n",
        "    '''\n",
        "    while True:\n",
        "        for i in range(0, len(encoder_inp), batch_size):\n",
        "            # creating empty matrix\n",
        "            enc_inp_batch = np.zeros(shape = (batch_size, encoder_inp.shape[-1])) # shape = (batch_size, max_len)\n",
        "            dec_inp_batch = np.zeros(shape = (batch_size, decoder_inp.shape[-1])) # shape = (batch_size, max_len)\n",
        "            dec_out_batch = np.zeros(shape = (batch_size, decoder_out.shape[-1])) # shape = (batch_size, max_len)\n",
        "            for j in range(batch_size):\n",
        "                if (i+j) < len(encoder_inp):\n",
        "                    # adding batch wise values\n",
        "                    enc_inp_batch[j] = encoder_inp[i+j]\n",
        "                    dec_inp_batch[j] = decoder_inp[i+j]\n",
        "                    dec_out_batch[j] = decoder_out[i+j]\n",
        "            # Yield is a keyword in Python that is used to return from a function without \n",
        "            # destroying the states of its local variable and when the function is called, \n",
        "            # the execution starts from the last yield statement.\n",
        "            yield [enc_inp_batch, dec_inp_batch], dec_out_batch"
      ],
      "metadata": {
        "id": "vyWBFP1tL2lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "ViYGwFOCOCb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Input, Dense, RNN, LSTMCell, Activation, add, concatenate\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "NPhfSRNCN8Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder - Decoder Seq-Seq model"
      ],
      "metadata": {
        "id": "OS2UfUDdODzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns encoder-outputs,encoder_final_state_h,encoder_final_state_c\n",
        "    '''\n",
        "\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "\n",
        "        super().__init__()\n",
        "        #Initialize Embedding layer\n",
        "        self.embedding = Embedding(inp_vocab_size, embedding_size, input_length = input_length)\n",
        "        #Intialize Encoder LSTM layer\n",
        "        self.lstm_size = lstm_size\n",
        "        lstmcell = LSTMCell(lstm_size)\n",
        "        self.lstm = RNN(lstmcell, return_sequences = True, return_state = True)\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          returns -- encoder_output, last time step's hidden and cell state\n",
        "        '''\n",
        "        embeddings = self.embedding(input_sequence)\n",
        "        \n",
        "        encoder_output, encoder_final_state_h, encoder_final_state_c = self.lstm(embeddings, initial_state = states)\n",
        "        \n",
        "        return encoder_output, encoder_final_state_h, encoder_final_state_c\n",
        "\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      '''\n",
        "      Given a batch size it will return intial hidden state and intial cell state.\n",
        "      '''\n",
        "      return tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))"
      ],
      "metadata": {
        "id": "XFkdLubQOFhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "\n",
        "    def __init__(self,out_vocab_size,embedding_size,lstm_size,input_length):\n",
        "        super().__init__()\n",
        "        self.lstm_size = lstm_size\n",
        "        #Initialize Embedding layer\n",
        "        self.embed_layer = Embedding(input_dim=out_vocab_size, output_dim=embedding_size, input_length=input_length)\n",
        "        #Intialize Decoder LSTM layer\n",
        "        lstmcell = LSTMCell(lstm_size)\n",
        "        self.lstm_layer = RNN(lstmcell, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,initial_states):\n",
        "        '''\n",
        "          This function takes a sequence input and the initial states of the encoder.\n",
        "          Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to decoder_lstm\n",
        "        \n",
        "          returns -- decoder_output,decoder_final_state_h,decoder_final_state_c\n",
        "        '''\n",
        "        x = self.embed_layer(input_sequence)\n",
        "        decoder_output,decoder_final_state_h,decoder_final_state_c = self.lstm_layer(x, initial_state = initial_states)\n",
        "        return decoder_output,decoder_final_state_h,decoder_final_state_c"
      ],
      "metadata": {
        "id": "tURlXWydOG0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jYwO2tMhOIka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmHNY69LigTu"
      },
      "outputs": [],
      "source": [
        "class Encoder_decoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, ita_vocab_size, eng_vocab_size, enc_embedding_size, dec_embedding_size, lstm_size,\n",
        "                 enc_input_length, dec_input_length):\n",
        "        super().__init__()\n",
        "        #Create encoder object\n",
        "        self.encode_obj = Encoder(ita_vocab_size,enc_embedding_size,lstm_size,enc_input_length)\n",
        "        #Create decoder object\n",
        "        self.decode_obj = Decoder(eng_vocab_size,dec_embedding_size,lstm_size,dec_input_length)\n",
        "        #Intialize Dense layer(out_vocab_size) with activation='softmax'\n",
        "        self.dense = Dense(eng_vocab_size, activation='softmax')\n",
        "        self.enc_input_length = enc_input_length\n",
        "    \n",
        "    def call(self,data):\n",
        "        '''\n",
        "        A. Pass the input sequence to Encoder layer -- Return encoder_output,encoder_final_state_h,encoder_final_state_c\n",
        "        B. Pass the target sequence to Decoder layer with intial states as encoder_final_state_h,encoder_final_state_C\n",
        "        C. Pass the decoder_outputs into Dense layer \n",
        "        \n",
        "        Return decoder_outputs\n",
        "        '''\n",
        "        enc_data = data[0]\n",
        "        dec_data = data[1]\n",
        "\n",
        "        # A\n",
        "        enc_initial_state = self.encode_obj.initialize_states(tf.shape(enc_data)[0])\n",
        "        enc_out, enc_h, enc_c = self.encode_obj(enc_data,enc_initial_state)\n",
        "\n",
        "        # B\n",
        "        dec_out, dec_h, dec_c = self.decode_obj(dec_data, [enc_h, enc_c])\n",
        "        # C\n",
        "        x = self.dense(dec_out)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_embedding_size = 50\n",
        "dec_embedding_size = 100 \n",
        "lstm_size = 128\n",
        "\n",
        "# defining model\n",
        "en_hi_model = Encoder_decoder(eng_vocab_size, hi_vocab_size, enc_embedding_size, dec_embedding_size, lstm_size,\n",
        "                 enc_input_length, dec_input_length)"
      ],
      "metadata": {
        "id": "5hdSVDcMOKr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model compile\n",
        "en_hi_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy')\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "# setting tensorboard\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# directory to save log\n",
        "log_dir='/content/logs/fit/en_hi_model/' \n",
        "\n",
        "# call back of tensorboard\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "callback = [tensorboard_callback, early_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90HsXT0dOQFS",
        "outputId": "4ee505ce-c322-4b1e-f0d4-7118249c4f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 444\n",
        "# send data to data generators\n",
        "train_data_generator = data_generator(train_eng_inp_seq, train_dec_in_seq, train_dec_out_seq, batch_size)\n",
        "val_data_generator = data_generator(test_eng_inp_seq, test_dec_in_seq, test_dec_out_seq, batch_size)\n",
        "\n",
        "# train model\n",
        "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHzoSeKPOZ_y",
        "outputId": "5940c668-3c42-4775-b67b-5e6a8e2e96e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 7s 69ms/step - loss: 4.1362 - val_loss: 2.5805\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 2.4765 - val_loss: 2.3100\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 2.2231 - val_loss: 2.1216\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 2.0946 - val_loss: 2.0311\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 2.0188 - val_loss: 1.9660\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 1.9579 - val_loss: 1.9106\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 1.9036 - val_loss: 1.8586\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 1.8479 - val_loss: 1.8051\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 1.7906 - val_loss: 1.7505\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 6s 83ms/step - loss: 1.7367 - val_loss: 1.6975\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd3071e4990>"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk1ZOnR9OiyH",
        "outputId": "2a20bde7-e167-4c66-9cac-6e4bec96065c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 5s 66ms/step - loss: 1.6841 - val_loss: 1.6521\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 1.6341 - val_loss: 1.6100\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 1.5878 - val_loss: 1.5654\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 1.5433 - val_loss: 1.5232\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 1.4976 - val_loss: 1.4826\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 8s 114ms/step - loss: 1.4529 - val_loss: 1.4404\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 10s 139ms/step - loss: 1.4074 - val_loss: 1.3994\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 10s 138ms/step - loss: 1.3630 - val_loss: 1.3608\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 8s 107ms/step - loss: 1.3211 - val_loss: 1.3289\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 1.2794 - val_loss: 1.2925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd1f4e60190>"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiT3Wx3iBM6J",
        "outputId": "af32fe82-90dd-497c-952a-db0c9675639f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "74/74 [==============================] - 5s 66ms/step - loss: 1.2409 - val_loss: 1.2524\n",
            "Epoch 2/10\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 1.2043 - val_loss: 1.2184\n",
            "Epoch 3/10\n",
            "74/74 [==============================] - 6s 76ms/step - loss: 1.1669 - val_loss: 1.1889\n",
            "Epoch 4/10\n",
            "74/74 [==============================] - 4s 61ms/step - loss: 1.1327 - val_loss: 1.1533\n",
            "Epoch 5/10\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 1.0985 - val_loss: 1.1252\n",
            "Epoch 6/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 1.0643 - val_loss: 1.0958\n",
            "Epoch 7/10\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 1.0313 - val_loss: 1.0687\n",
            "Epoch 8/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 1.0012 - val_loss: 1.0404\n",
            "Epoch 9/10\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.9700 - val_loss: 1.0089\n",
            "Epoch 10/10\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.9384 - val_loss: 0.9823\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd30e6ebe90>"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 20, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owHsNYQaBvR6",
        "outputId": "ca0bad9f-f6f1-47f9-9193-f9dc7d398afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "74/74 [==============================] - 8s 105ms/step - loss: 0.9075 - val_loss: 0.9562\n",
            "Epoch 2/20\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.8780 - val_loss: 0.9276\n",
            "Epoch 3/20\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.8466 - val_loss: 0.9016\n",
            "Epoch 4/20\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.8171 - val_loss: 0.8703\n",
            "Epoch 5/20\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.7864 - val_loss: 0.8437\n",
            "Epoch 6/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.7566 - val_loss: 0.8149\n",
            "Epoch 7/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.7281 - val_loss: 0.7910\n",
            "Epoch 8/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.7027 - val_loss: 0.7712\n",
            "Epoch 9/20\n",
            "74/74 [==============================] - 4s 61ms/step - loss: 0.6760 - val_loss: 0.7469\n",
            "Epoch 10/20\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.6503 - val_loss: 0.7218\n",
            "Epoch 11/20\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.6261 - val_loss: 0.6994\n",
            "Epoch 12/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.6010 - val_loss: 0.6766\n",
            "Epoch 13/20\n",
            "74/74 [==============================] - 5s 66ms/step - loss: 0.5770 - val_loss: 0.6538\n",
            "Epoch 14/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.5556 - val_loss: 0.6335\n",
            "Epoch 15/20\n",
            "74/74 [==============================] - 4s 58ms/step - loss: 0.5340 - val_loss: 0.6157\n",
            "Epoch 16/20\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.5130 - val_loss: 0.5964\n",
            "Epoch 17/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.4925 - val_loss: 0.5763\n",
            "Epoch 18/20\n",
            "74/74 [==============================] - 5s 64ms/step - loss: 0.4726 - val_loss: 0.5581\n",
            "Epoch 19/20\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.4541 - val_loss: 0.5410\n",
            "Epoch 20/20\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.4352 - val_loss: 0.5256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd30e6ebd10>"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_hi_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 30, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M50bj_WXCnyd",
        "outputId": "ef236df8-3e48-4db3-fbb7-5fccb27226a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "74/74 [==============================] - 8s 112ms/step - loss: 0.4192 - val_loss: 0.5131\n",
            "Epoch 2/30\n",
            "74/74 [==============================] - 6s 79ms/step - loss: 0.4019 - val_loss: 0.4931\n",
            "Epoch 3/30\n",
            "74/74 [==============================] - 4s 61ms/step - loss: 0.3846 - val_loss: 0.4777\n",
            "Epoch 4/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.3683 - val_loss: 0.4629\n",
            "Epoch 5/30\n",
            "74/74 [==============================] - 4s 60ms/step - loss: 0.3519 - val_loss: 0.4481\n",
            "Epoch 6/30\n",
            "74/74 [==============================] - 5s 64ms/step - loss: 0.3366 - val_loss: 0.4340\n",
            "Epoch 7/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.3224 - val_loss: 0.4242\n",
            "Epoch 8/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.3089 - val_loss: 0.4105\n",
            "Epoch 9/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.2964 - val_loss: 0.3986\n",
            "Epoch 10/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.2845 - val_loss: 0.3892\n",
            "Epoch 11/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.2712 - val_loss: 0.3755\n",
            "Epoch 12/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.2602 - val_loss: 0.3639\n",
            "Epoch 13/30\n",
            "74/74 [==============================] - 5s 64ms/step - loss: 0.2496 - val_loss: 0.3555\n",
            "Epoch 14/30\n",
            "74/74 [==============================] - 5s 64ms/step - loss: 0.2402 - val_loss: 0.3487\n",
            "Epoch 15/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.2293 - val_loss: 0.3390\n",
            "Epoch 16/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.2195 - val_loss: 0.3293\n",
            "Epoch 17/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.2105 - val_loss: 0.3193\n",
            "Epoch 18/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.2013 - val_loss: 0.3120\n",
            "Epoch 19/30\n",
            "74/74 [==============================] - 4s 61ms/step - loss: 0.1935 - val_loss: 0.3055\n",
            "Epoch 20/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.1855 - val_loss: 0.2982\n",
            "Epoch 21/30\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.1788 - val_loss: 0.2927\n",
            "Epoch 22/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.1726 - val_loss: 0.2873\n",
            "Epoch 23/30\n",
            "74/74 [==============================] - 5s 69ms/step - loss: 0.1659 - val_loss: 0.2818\n",
            "Epoch 24/30\n",
            "74/74 [==============================] - 5s 64ms/step - loss: 0.1592 - val_loss: 0.2752\n",
            "Epoch 25/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.1534 - val_loss: 0.2705\n",
            "Epoch 26/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.1492 - val_loss: 0.2662\n",
            "Epoch 27/30\n",
            "74/74 [==============================] - 5s 63ms/step - loss: 0.1435 - val_loss: 0.2612\n",
            "Epoch 28/30\n",
            "74/74 [==============================] - 5s 62ms/step - loss: 0.1375 - val_loss: 0.2581\n",
            "Epoch 29/30\n",
            "74/74 [==============================] - 5s 61ms/step - loss: 0.1328 - val_loss: 0.2516\n",
            "Epoch 30/30\n",
            "74/74 [==============================] - 4s 59ms/step - loss: 0.1283 - val_loss: 0.2469\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd207a891d0>"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_hi_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZMGOdDNQ9He",
        "outputId": "c141ff6e-39ce-4ae3-8b5c-c4cefbb51a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_decoder_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_2 (Encoder)         multiple                  241798    \n",
            "                                                                 \n",
            " decoder_2 (Decoder)         multiple                  439048    \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  415122    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,095,968\n",
            "Trainable params: 1,095,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference: query resolution aaic team \n",
        "def predict(input_sentence):\n",
        "    \n",
        "    '''\n",
        "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to decoder\n",
        "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "            predicted_out,state_h,state_c=model.layers[1](dec_input,states)\n",
        "            pass the predicted_out to the dense layer\n",
        "            update the states=[state_h,state_c]\n",
        "            And get the index of the word with maximum probability of the dense layer output, using the tokenizer(word index) get the word and then store it in a string.\n",
        "            Update the input_to_decoder with current predictions\n",
        "    F. Return the predicted sentence\n",
        "    '''\n",
        "    # A\n",
        "    # lets tokenize the sentence first\n",
        "    tokenized_encoder_input = token_eng.texts_to_sequences([input_sentence])\n",
        "    # padding the sequence\n",
        "    encoder_input = pad_sequences(tokenized_encoder_input, maxlen=max_len, padding='post', dtype='int32')\n",
        "    \n",
        "    # B\n",
        "    # get the initial encoder states\n",
        "    enc_init_states = en_hi_model.layers[0].initialize_states(1)\n",
        "    enc_out, enc_h_state, enc_c_state = en_hi_model.layers[0](encoder_input, states = enc_init_states)\n",
        "\n",
        "    # C\n",
        "    decoder_initial_states = [enc_h_state, enc_c_state]\n",
        "    decoder_initial_input = np.zeros((1,1))\n",
        "    decoder_initial_input[0,0] = hi_word_index['<start>']\n",
        "\n",
        "    # D\n",
        "    predicted_words = []\n",
        "    predicting = True\n",
        "    while predicting:\n",
        "\n",
        "        dec_out, dec_h_state, dec_c_state = en_hi_model.layers[1](decoder_initial_input, decoder_initial_states)\n",
        "        english_predicted_int = np.argmax(en_hi_model.layers[2](dec_out).numpy().ravel())\n",
        "        predicted_words.append(english_predicted_int)\n",
        "        # replacing the next input to decoder with current decoder output\n",
        "        decoder_initial_input[0,0] = english_predicted_int\n",
        "        # replacing next decoder initial states with current decoder output states\n",
        "        decoder_initial_states = [dec_h_state, dec_c_state]\n",
        "\n",
        "        # end condition\n",
        "        if english_predicted_int == hi_word_index['<end>'] or len(predicted_words) >= 20:\n",
        "            break\n",
        "\n",
        "    # F\n",
        "    # remove <end> from end\n",
        "    predicted_words = predicted_words[:-1]\n",
        "    return ' '.join([hi_index_word.get(ele, '') for ele in predicted_words])"
      ],
      "metadata": {
        "id": "ZQyjiwHQSKAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "bleu_score = []\n",
        "print(\"=\" * 50)     \n",
        "\n",
        "# sampling 1000 datapoints randomly from test set\n",
        "for index, (_, row) in enumerate(train.sample(1000).iterrows()):\n",
        "    input_sent = row.english\n",
        "    predicted_eng = predict(input_sent)\n",
        "    actual_eng = row.hi_out.replace('<end>','').strip()\n",
        "\n",
        "    # printing Translation Pairs\n",
        "    if (index + 1)%100 == 0:\n",
        "        print(f\"\\English sentence: {input_sent}\")\n",
        "        print(f\"Actual Translation: {actual_eng}\")\n",
        "        print(f\"Predicted Translation: {predicted_eng}\\n\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    bleu_score.append(sentence_bleu([actual_eng.split(),], predicted_eng.split()))\n",
        "\n",
        "print(f\"Mean Bleu Score = {np.mean(bleu_score)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyWXDycPSy-C",
        "outputId": "fe7a7ae9-4858-46fa-ba8a-5bcb8b68fbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "\\English sentence: menu bar\n",
            "Actual Translation: मेन्यू पट्टी\n",
            "Predicted Translation: मेन्यू पट्टी\n",
            "\n",
            "==================================================\n",
            "\\English sentence: the name of the device\n",
            "Actual Translation: युक्ति का नाम\n",
            "Predicted Translation: युक्ति का नाम\n",
            "\n",
            "==================================================\n",
            "\\English sentence: start depth\n",
            "Actual Translation: गहराई आरंभ करें\n",
            "Predicted Translation: गहराई आरंभ करें\n",
            "\n",
            "==================================================\n",
            "\\English sentence: path\n",
            "Actual Translation: \n",
            "Predicted Translation: पथ\n",
            "\n",
            "==================================================\n",
            "\\English sentence: shell values to watch\n",
            "Actual Translation: शेल को\n",
            "Predicted Translation: शेल को\n",
            "\n",
            "==================================================\n",
            "\\English sentence: ruler\n",
            "Actual Translation: रूलर\n",
            "Predicted Translation: रूलर\n",
            "\n",
            "==================================================\n",
            "\\English sentence: selected uri in the file manager plugin\n",
            "Actual Translation: चयनित यूआरआई इंच फ़ाइल प्लगइन\n",
            "Predicted Translation: चयनित यूआरआई इंच फ़ाइल प्लगइन\n",
            "\n",
            "==================================================\n",
            "\\English sentence: rename\n",
            "Actual Translation: नाम बदलें\n",
            "Predicted Translation: नाम बदलें\n",
            "\n",
            "==================================================\n",
            "\\English sentence: the actor s position on the z axis\n",
            "Actual Translation: अक्ष पर कर्ता की स्थिति\n",
            "Predicted Translation: अक्ष पर कर्ता की स्थिति\n",
            "\n",
            "==================================================\n",
            "\\English sentence: alphabet selection\n",
            "Actual Translation: वर्णमाला चयन\n",
            "Predicted Translation: वर्णमाला चयन\n",
            "\n",
            "==================================================\n",
            "Mean Bleu Score = 0.24614254136267805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention models"
      ],
      "metadata": {
        "id": "Bv-H4YMCaG9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Encoder model -- That takes a input sequence and returns output sequence\n",
        "    '''\n",
        "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
        "        super().__init__()\n",
        "        #Initialize Embedding layer\n",
        "        self.embedding = Embedding(inp_vocab_size, embedding_size, input_length=input_length)\n",
        "        #Intialize Encoder LSTM layer\n",
        "        self.lstm_size = lstm_size\n",
        "        lstmcell = LSTMCell(lstm_size)\n",
        "        self.lstm = RNN(lstmcell, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "        '''\n",
        "            This function takes a sequence input and the initial states of the encoder.\n",
        "            Pass the input_sequence input to the Embedding layer, Pass the embedding layer ouput to encoder_lstm\n",
        "            returns -- All encoder_outputs, last time steps hidden and cell state\n",
        "        '''\n",
        "        embed = self.embedding(input_sequence)\n",
        "        enc_out, enc_h, enc_c = self.lstm(embed, initial_state=states)\n",
        "        return enc_out, enc_h, enc_c\n",
        "\n",
        "    def initialize_states(self, batch_size):\n",
        "        '''\n",
        "            Given a batch size it will return intial hidden state and intial cell state.\n",
        "        '''\n",
        "        # we require tensor if we return numpy array it gives error\n",
        "        return tf.zeros((batch_size, self.lstm_size)), tf.zeros((batch_size, self.lstm_size))"
      ],
      "metadata": {
        "id": "M4aJgUOgSy7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    '''\n",
        "        Class that calculates similarity score based on the scoring_function using Bahdanu attention mechanism.\n",
        "    '''\n",
        "    def __init__(self,scoring_function, att_units):\n",
        "        # Please go through the reference notebook and research paper to complete the scoring functions\n",
        "        super().__init__()\n",
        "        self.scoring_function = scoring_function\n",
        "        if scoring_function == 'concat':\n",
        "            # Intialize variables needed for Concat score function here\n",
        "            # add dense layer for finding w1, w2 and V\n",
        "            self.tanh_activation = Activation('tanh')\n",
        "            self.dense_concat_1 = Dense(att_units)\n",
        "            self.dense_concat_2 = Dense(att_units)\n",
        "            self.dense_1 = Dense(1)\n",
        "        elif scoring_function == 'general':\n",
        "            # Intialize variables needed for General score function here\n",
        "            self.dense_general = Dense(att_units)      \n",
        "        \n",
        "    def call(self,decoder_hidden_state,encoder_output):\n",
        "        '''\n",
        "            Attention mechanism takes two inputs current step -- decoder_hidden_state and all the encoder_outputs.\n",
        "            * Based on the scoring function we will find the score or similarity between decoder_hidden_state and encoder_output.\n",
        "            Multiply the score function with your encoder_outputs to get the context vector.\n",
        "            Function returns context vector and attention weights(softmax - scores)\n",
        "        '''\n",
        "        if self.scoring_function == 'dot':\n",
        "            # Implement Dot score function here\n",
        "            # decoder_hidden_state = (16, 32)\n",
        "            decoder_hidden_state = tf.expand_dims(decoder_hidden_state, -1)\n",
        "            # decoder_hidden_state = (16, 32, 1)\n",
        "            # mul encoder = (16, 10, 32) and (16, 32, 1)\n",
        "            alpha = tf.matmul(encoder_output, decoder_hidden_state)\n",
        "            # we get alpha of shape (16, 10, 1)\n",
        "\n",
        "        elif self.scoring_function == 'concat':\n",
        "            # tanh((Hd(t-1) * W1 + He(t=n) * W2)) * V\n",
        "            # Implement General score function here\n",
        "            transformed_enc_out = self.dense_concat_1(encoder_output)\n",
        "            transformed_dec_hidden_state = self.dense_concat_2(decoder_hidden_state)\n",
        "\n",
        "            added_both = add([transformed_enc_out, tf.expand_dims(transformed_dec_hidden_state,1)])\n",
        "            added_both = self.tanh_activation(added_both)\n",
        "            alpha = self.dense_1(added_both)\n",
        "            # we get alpha of shape (16, 10, 1)\n",
        "\n",
        "        elif self.scoring_function == 'general':\n",
        "            # He(n,d) * W (d,d') * Hd(d',1)\n",
        "            # Implement General score function here\n",
        "            # pass encoder (16, 10, 32) to dense layer \n",
        "            transformed_enc_out = self.dense_general(encoder_output)\n",
        "            decoder_hidden_state = tf.expand_dims(decoder_hidden_state, -1)\n",
        "\n",
        "            alpha = tf.matmul(transformed_enc_out, decoder_hidden_state)\n",
        "            # we get alpha of shape (16, 10, 1)\n",
        "\n",
        "        # apply softmax on alphas\n",
        "        alpha = tf.squeeze(alpha, axis = -1)\n",
        "        attention_weights = Activation('softmax')(alpha)\n",
        "        # expand dimension of alpha to do matrix multiplication with encoder \n",
        "        attention_weights = tf.expand_dims(attention_weights, axis = -1)\n",
        "        \n",
        "        context_vector = tf.matmul(tf.transpose(encoder_output, perm = [0,2,1]), attention_weights)\n",
        "        # remove extra dimension\n",
        "        context_vector = tf.squeeze(context_vector, axis = -1)\n",
        "\n",
        "        return context_vector, attention_weights    "
      ],
      "metadata": {
        "id": "Uu4gZrLdSy4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    Class for finding translation word by word\n",
        "    '''\n",
        "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
        "        super().__init__()\n",
        "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
        "        self.embedding = Embedding(tar_vocab_size, embedding_dim, input_length=input_length)\n",
        "        lstmcell = LSTMCell(dec_units)\n",
        "        self.lstm = RNN(lstmcell, return_sequences=False, return_state=True)\n",
        "        self.dense = Dense(tar_vocab_size)\n",
        "        self.attention = Attention(score_fun, att_units)\n",
        "\n",
        "    def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
        "        '''\n",
        "            One step decoder mechanisim step by step:\n",
        "        A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "        B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "        C. Concat the context vector with the step A output\n",
        "        D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "        E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "        F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "        '''\n",
        "        # if this parameters then we get shapes of following\n",
        "        # tar_vocab_size=13 \n",
        "        # embedding_dim=12 \n",
        "        # input_length=10\n",
        "        # dec_units=16 \n",
        "        # att_units=16\n",
        "        # batch_size=32\n",
        "\n",
        "        # A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
        "        embeddings_input_dec = self.embedding(input_to_decoder)  #shape = (32, 1, 12)\n",
        "        \n",
        "        # B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
        "        context_vector, att_weights = self.attention(state_h, encoder_output)  # context_vector = (32, 16)\n",
        "\n",
        "        # C. Concat the context vector with the step A output\n",
        "        input_to_decoder = concatenate([embeddings_input_dec,tf.expand_dims(context_vector,1)])  #shape = (32, 1, 16)\n",
        "\n",
        "        # D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
        "        dec_out, dec_h_state, dec_c_state = self.lstm(input_to_decoder, initial_state=[state_h, state_c])\n",
        "\n",
        "        # E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
        "        predicted_out = self.dense(dec_out)\n",
        "\n",
        "        # output ,state_h ,state_c\n",
        "        # (32, 13) (32, 16) (32, 16)\n",
        "\n",
        "        # F. Return the states from step D, output from Step E, attention weights from Step -B\n",
        "        return predicted_out, dec_h_state, dec_c_state, att_weights, context_vector"
      ],
      "metadata": {
        "id": "tqImeIlfSy1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_size, input_length, dec_units ,score_fun ,att_units):\n",
        "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
        "        super().__init__()\n",
        "        self.onestepdecoder = One_Step_Decoder(out_vocab_size, embedding_size, input_length, dec_units, score_fun, att_units)\n",
        "\n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "        #Initialize an empty Tensor array, that will store the outputs at each and every time step\n",
        "        #Create a tensor array as shown in the reference notebook\n",
        "        # https://www.tensorflow.org/api_docs/python/tf/TensorArray\n",
        "        all_outputs = tf.TensorArray(tf.float32, size = tf.shape(input_to_decoder)[1])\n",
        "\n",
        "        #Iterate till the length of the decoder input\n",
        "        for i in range(tf.shape(input_to_decoder)[1]):\n",
        "            # Call onestepdecoder for each token in decoder_input\n",
        "            output, decoder_hidden_state, decoder_cell_state, attention_weights, context_vector = \\\n",
        "            self.onestepdecoder(input_to_decoder[:,i:i+1], encoder_output,decoder_hidden_state, decoder_cell_state)\n",
        "            # Store the all_outputs in tensorarray\n",
        "            all_outputs = all_outputs.write(i, output)\n",
        "        # Return the tensor \n",
        "        all_outputs = tf.transpose(all_outputs.stack(), perm = [1,0,2])\n",
        "        return all_outputs"
      ],
      "metadata": {
        "id": "13JZFYhqSyzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_size, enc_lstm_units, dec_lstm_units, enc_input_length, dec_input_length, \\\n",
        "                 score_fun, att_units):\n",
        "        super().__init__()\n",
        "        #Intializing objects from encoder decoder\n",
        "        self.encoder = Encoder(inp_vocab_size, embedding_size, enc_lstm_units, enc_input_length)\n",
        "        self.decoder = Decoder(out_vocab_size, embedding_size, dec_input_length, dec_lstm_units, score_fun, att_units)\n",
        "\n",
        "    def call(self,data):\n",
        "        encoder_inputs = data[0]\n",
        "        decoder_inputs = data[1]\n",
        "        #Intialize encoder states, Pass the encoder_sequence to the embedding layer\n",
        "        encoder_initial_states = self.encoder.initialize_states(tf.shape(encoder_inputs)[0])\n",
        "        enc_out, enc_h_state, enc_c_state = self.encoder(encoder_inputs, encoder_initial_states)\n",
        "        # Decoder initial states are encoder final states, Initialize it accordingly\n",
        "        # Pass the decoder sequence,encoder_output,decoder states to Decoder\n",
        "        dec_out = self.decoder(decoder_inputs, enc_out, enc_h_state, enc_c_state)\n",
        "\n",
        "        # return the decoder output\n",
        "        return dec_out"
      ],
      "metadata": {
        "id": "oQjdt5bYaYOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.tensorflow.org/tutorials/text/image_captioning#model\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Custom loss function that will not consider the loss for padded zeros.\n",
        "    why are we using this, can't we use simple sparse categorical crossentropy?\n",
        "    Yes, you can use simple sparse categorical crossentropy as loss like we did in task-1. But in this loss function we are ignoring the loss\n",
        "    for the padded zeros. i.e when the input is zero then we donot need to worry what the output is. This padded zeros are added from our end\n",
        "    during preprocessing to make equal length for all the sentences.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "4KFKtzOTaaRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dot model"
      ],
      "metadata": {
        "id": "__rt7C38bqHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the model\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "dot_model = encoder_decoder(inp_vocab_size = eng_vocab_size, out_vocab_size = hi_vocab_size,\n",
        "                            embedding_size = 128, enc_lstm_units = 128, dec_lstm_units = 128,\n",
        "                            enc_input_length = enc_input_length, dec_input_length = dec_input_length, \n",
        "                            score_fun = 'dot', att_units = 128)\n",
        "#compiling the model\n",
        "dot_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002), loss = loss_function)\n",
        "\n",
        "# callbacks\n",
        "# EarlyStopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "# setting tensorboard\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# directory to save log\n",
        "log_dir='/content/logs/fit/dot_model/' \n",
        "\n",
        "# call back of tensorboard\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "callback = [tensorboard_callback, early_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbD_PgXwabx3",
        "outputId": "ba338137-73c4-4dc7-d679-51ccebce2aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 224\n",
        "# send data to data generators\n",
        "train_data_generator = data_generator(train_eng_inp_seq, train_dec_in_seq, train_dec_out_seq, batch_size)\n",
        "val_data_generator = data_generator(test_eng_inp_seq, test_dec_in_seq, test_dec_out_seq, batch_size)\n",
        "\n",
        "# model training\n",
        "dot_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 10, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bgfkzumady9",
        "outputId": "2939a72e-b22b-42d7-d5ae-cbe42dce5349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "147/147 [==============================] - 31s 191ms/step - loss: 2.2770 - val_loss: 2.1405\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 17s 119ms/step - loss: 2.0638 - val_loss: 1.9783\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 23s 158ms/step - loss: 1.8822 - val_loss: 1.7713\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 25s 172ms/step - loss: 1.6563 - val_loss: 1.5548\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 21s 140ms/step - loss: 1.4436 - val_loss: 1.3595\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 26s 177ms/step - loss: 1.2324 - val_loss: 1.1531\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 28s 189ms/step - loss: 1.0208 - val_loss: 0.9653\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 26s 177ms/step - loss: 0.8303 - val_loss: 0.8000\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 22s 147ms/step - loss: 0.6689 - val_loss: 0.6683\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 21s 143ms/step - loss: 0.5376 - val_loss: 0.5566\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd30731bd90>"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "dot_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 35, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "k5yhssswDxqr",
        "outputId": "d3bbfc8d-2029-4d5f-ac3c-e15128c5551b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "147/147 [==============================] - 18s 119ms/step - loss: 0.4315 - val_loss: 0.4673\n",
            "Epoch 2/35\n",
            "147/147 [==============================] - 29s 196ms/step - loss: 0.3496 - val_loss: 0.4017\n",
            "Epoch 3/35\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.2864 - val_loss: 0.3497\n",
            "Epoch 4/35\n",
            "147/147 [==============================] - 16s 109ms/step - loss: 0.2373 - val_loss: 0.3059\n",
            "Epoch 5/35\n",
            "147/147 [==============================] - 15s 102ms/step - loss: 0.1984 - val_loss: 0.2748\n",
            "Epoch 6/35\n",
            "147/147 [==============================] - 16s 110ms/step - loss: 0.1692 - val_loss: 0.2520\n",
            "Epoch 7/35\n",
            " 76/147 [==============>...............] - ETA: 6s - loss: 0.1511"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-88fd81a29e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdot_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data_generator\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eng_inp_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eng_inp_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dot_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0_gNuFmalAF",
        "outputId": "a44c43ae-1822-42d6-c49e-2d6ab2393d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder_decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  515968    \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  1024146   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,540,114\n",
            "Trainable params: 1,540,114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refer: https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\n",
        "def plot_attention(attention, encoder_inp, predicted):\n",
        "    heatmap_df = pd.DataFrame(attention, columns=encoder_inp, index=predicted)\n",
        "    plt.figure(figsize=(7, 10))\n",
        "    sns.heatmap(heatmap_df, cmap='YlGnBu', linewidths=.3)\n",
        "    plt.title(\"Attention Plot\")\n",
        "    plt.ylabel(\"English\")\n",
        "    plt.xlabel(\"Italian\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "c88DYNNRavzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_attention(input_sentence, model, plot_attention_weights = False):\n",
        "    '''\n",
        "    A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "    B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "    C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
        "    D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "            predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
        "            Save the attention weights\n",
        "            And get the word using the tokenizer(word index) and then store it in a string.\n",
        "    E. Call plot_attention(#params)\n",
        "    F. Return the predicted sentence\n",
        "    '''\n",
        "    # A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
        "    # tokenization of the sentence \n",
        "    tokenized_encoder_input = token_eng.texts_to_sequences([input_sentence])\n",
        "    # padding the sequence\n",
        "    encoder_input = pad_sequences(tokenized_encoder_input, maxlen=max_len, padding='post')\n",
        "\n",
        "    # B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
        "    enc_init_states = model.layers[0].initialize_states(1)\n",
        "    enc_out, enc_h_state, enc_c_state = model.layers[0](encoder_input, states = enc_init_states)\n",
        "    # initializing decoder states\n",
        "    decoder_h_state = enc_h_state\n",
        "    decoder_c_state = enc_c_state\n",
        "\n",
        "    # C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
        "    # decoder initial input\n",
        "    decoder_initial_input = np.zeros((1,1))\n",
        "    decoder_initial_input[0,0] = hi_word_index['<start>']\n",
        "\n",
        "    # D. till we reach max_length of decoder or till the model predicted word <end>:\n",
        "    predicted_words = []\n",
        "    att_weights_all = []\n",
        "    predicting = True\n",
        "    while predicting:\n",
        "        # predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
        "        prediction, decoder_h_state, decoder_c_state, att_weights,_ = model.layers[1].onestepdecoder(decoder_initial_input, \n",
        "                                                                                                                  enc_out, \n",
        "                                                                                                                  decoder_h_state,\n",
        "                                                                                                                  decoder_c_state)\n",
        "        #predicted english token\n",
        "        english_predicted_int = np.argmax(prediction.numpy().ravel())\n",
        "        predicted_words.append(english_predicted_int)\n",
        "        \n",
        "        # Save the attention weights\n",
        "        att_weights_all.append(att_weights.numpy().ravel())\n",
        "        decoder_initial_input[0,0] = english_predicted_int\n",
        "        # break condition\n",
        "        if english_predicted_int == hi_word_index['<end>'] or len(predicted_words)>=20:\n",
        "            break\n",
        "    \n",
        "    #checking for non-padding words in encoder input\n",
        "    att_weights_all = np.array(att_weights_all)\n",
        "    non_padded_encoder_input = np.where(encoder_input[0] != 0)[0]\n",
        "    encoder_input_words = np.array(input_sentence.split())[non_padded_encoder_input]\n",
        "    # get the word using the tokenizer(word index) and then store it in a string.\n",
        "    decoder_output_words = [hi_index_word.get(ele, '') for ele in predicted_words]\n",
        "    #keeping only those attention weights corresponding to non-padded words\n",
        "    att_weights_all = att_weights_all[:,non_padded_encoder_input]\n",
        "\n",
        "    # E. Call plot_attention(#params)\n",
        "    if plot_attention_weights:\n",
        "        plot_attention(att_weights_all, encoder_input_words, decoder_output_words)\n",
        "    else:\n",
        "        # F. Return the predicted sentence\n",
        "        return ' '.join(decoder_output_words)"
      ],
      "metadata": {
        "id": "lO3uw5_3ayNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "bleu_score = []\n",
        "print(\"=\" * 50)     \n",
        "\n",
        "# sampling 1000 datapoints randomly from test set\n",
        "for index, (_, row) in enumerate(train.sample(1000).iterrows()):\n",
        "    input_sent = row.english\n",
        "    predicted_eng = predict_attention(input_sent, dot_model, plot_attention_weights = False)\n",
        "    actual_eng = row.hi_out.replace('<end>','').strip()\n",
        "    predicted_eng = predicted_eng.replace('<end>','').strip()\n",
        "\n",
        "    # printing Translation Pairs\n",
        "    if (index + 1)%100 == 0:\n",
        "        print(f\"\\English sentence: {input_sent}\")\n",
        "        print(f\"Actual Translation: {actual_eng}\")\n",
        "        print(f\"Predicted Translation: {predicted_eng}\\n\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    bleu_score.append(sentence_bleu([actual_eng.split(),], predicted_eng.split()))\n",
        "\n",
        "print(f\"Mean Bleu Score = {np.mean(bleu_score)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj8-7Rqqa-N2",
        "outputId": "a0780a26-2a2c-432b-ae48-966f539f99b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "\\English sentence: accessible table column header\n",
            "Actual Translation: पहुँच योग्य सारणी स्तंभ शीर्षिका\n",
            "Predicted Translation: पहुँच योग्य सारणी स्तंभ शीर्षिका\n",
            "\n",
            "==================================================\n",
            "\\English sentence: add a library to a target\n",
            "Actual Translation: जोड़ें को लक्ष्य\n",
            "Predicted Translation: जोड़ें को लक्ष्य\n",
            "\n",
            "==================================================\n",
            "\\English sentence: remove recently used project from list\n",
            "Actual Translation: नहीं प्रयुक्त संसाधन पाया के साथ यूआरआई से\n",
            "Predicted Translation: नहीं छवि चयनित फ़ाइल हटाएँ\n",
            "\n",
            "==================================================\n",
            "\\English sentence: browse api pages\n",
            "Actual Translation: पृष्ठ ब्राउज़ करें\n",
            "Predicted Translation: पृष्ठ ब्राउज़ करें\n",
            "\n",
            "==================================================\n",
            "\\English sentence: vala source file\n",
            "Actual Translation: वाला स्रोत फ़ाइल\n",
            "Predicted Translation: वाला स्रोत फ़ाइल\n",
            "\n",
            "==================================================\n",
            "\\English sentence: add new resource list\n",
            "Actual Translation: नई संसाधन सूची जोड़ें\n",
            "Predicted Translation: नई संसाधन सूची जोड़ें\n",
            "\n",
            "==================================================\n",
            "\\English sentence: create an svcd\n",
            "Actual Translation: एसवीसीडी बनाएँ\n",
            "Predicted Translation: एसवीसीडी बनाएँ\n",
            "\n",
            "==================================================\n",
            "\\English sentence: s element could not be created\n",
            "Actual Translation: तत्व को बनाया नहीं जा सका\n",
            "Predicted Translation: तत्व को बनाया नहीं जा सका\n",
            "\n",
            "==================================================\n",
            "\\English sentence: the two of hearts\n",
            "Actual Translation: लाल पान का दुक्का\n",
            "Predicted Translation: लाल पान का दुक्का\n",
            "\n",
            "==================================================\n",
            "\\English sentence: cd dvd creator\n",
            "Actual Translation: सर्जक\n",
            "Predicted Translation: सर्जक\n",
            "\n",
            "==================================================\n",
            "Mean Bleu Score = 0.301775923272452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General Model"
      ],
      "metadata": {
        "id": "SCCiuVwSD8TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the model\n",
        "tf.keras.backend.clear_session()\n",
        "general_model = encoder_decoder(inp_vocab_size = eng_vocab_size, out_vocab_size = hi_vocab_size,\n",
        "                            embedding_size = 128, enc_lstm_units = 128, dec_lstm_units = 128,\n",
        "                            enc_input_length = enc_input_length, dec_input_length = dec_input_length, \n",
        "                            score_fun = 'general', att_units = 128)\n",
        "#compiling the model\n",
        "general_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002), loss = loss_function)\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "# setting tensorboard\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "%load_ext tensorboard\n",
        "\n",
        "# directory to save log\n",
        "log_dir='/content/logs/fit/general_model/' \n",
        "\n",
        "# call back of tensorboard\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "callback = [tensorboard_callback, early_stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAGjB_09cVWV",
        "outputId": "bbb266d5-f1ca-4b4a-e42c-7352ba480c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "general_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 20, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8WSMr4PED1K",
        "outputId": "55df3420-76ca-4375-9dc9-56f38304fd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "147/147 [==============================] - 28s 161ms/step - loss: 2.2893 - val_loss: 2.1510\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 17s 112ms/step - loss: 2.0596 - val_loss: 1.9633\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 15s 104ms/step - loss: 1.8629 - val_loss: 1.7616\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 15s 104ms/step - loss: 1.6604 - val_loss: 1.5782\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 15s 104ms/step - loss: 1.4825 - val_loss: 1.4134\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 15s 104ms/step - loss: 1.3089 - val_loss: 1.2562\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 15s 102ms/step - loss: 1.1423 - val_loss: 1.1042\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.9835 - val_loss: 0.9596\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.8357 - val_loss: 0.8342\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 15s 105ms/step - loss: 0.7072 - val_loss: 0.7253\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 15s 102ms/step - loss: 0.5951 - val_loss: 0.6257\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.5012 - val_loss: 0.5501\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 15s 104ms/step - loss: 0.4245 - val_loss: 0.4812\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 16s 109ms/step - loss: 0.3579 - val_loss: 0.4250\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.3046 - val_loss: 0.3803\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 15s 102ms/step - loss: 0.2620 - val_loss: 0.3464\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 16s 106ms/step - loss: 0.2277 - val_loss: 0.3123\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.1966 - val_loss: 0.2879\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 15s 103ms/step - loss: 0.1733 - val_loss: 0.2686\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 15s 101ms/step - loss: 0.1527 - val_loss: 0.2503\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd311a7b6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "bleu_score = []\n",
        "print(\"=\" * 50)     \n",
        "\n",
        "# sampling 1000 datapoints randomly from test set\n",
        "for index, (_, row) in enumerate(train.sample(1000).iterrows()):\n",
        "    input_sent = row.english\n",
        "    predicted_eng = predict_attention(input_sent, general_model, plot_attention_weights = False)\n",
        "    actual_eng = row.hi_out.replace('<end>','').strip()\n",
        "    predicted_eng = predicted_eng.replace('<end>','').strip()\n",
        "\n",
        "    # printing Translation Pairs\n",
        "    if (index + 1)%100 == 0:\n",
        "        print(f\"\\English sentence: {input_sent}\")\n",
        "        print(f\"Actual Translation: {actual_eng}\")\n",
        "        print(f\"Predicted Translation: {predicted_eng}\\n\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    bleu_score.append(sentence_bleu([actual_eng.split(),], predicted_eng.split()))\n",
        "\n",
        "print(f\"Mean Bleu Score = {np.mean(bleu_score)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjWNvUW1EN5I",
        "outputId": "96339feb-0fc2-45fe-814f-a30e368e3c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "\\English sentence: running…\n",
            "Actual Translation: चल रहा है\n",
            "Predicted Translation: चल रहा है\n",
            "\n",
            "==================================================\n",
            "\\English sentence: profile plugins\n",
            "Actual Translation: प्रोफाइल प्लगिन\n",
            "Predicted Translation: प्रोफाइल प्लगिन\n",
            "\n",
            "==================================================\n",
            "\\English sentence: the position of the remote video window\n",
            "Actual Translation: दूरस्थ वीडियो विंडो की स्थिति\n",
            "Predicted Translation: दूरस्थ वीडियो विंडो की स्थिति\n",
            "\n",
            "==================================================\n",
            "\\English sentence: prefer vim emacs modelines over indentation settings\n",
            "Actual Translation: वरीयता\n",
            "Predicted Translation: वरीयता\n",
            "\n",
            "==================================================\n",
            "\\English sentence: caribou preferences\n",
            "Actual Translation: प्राथमिकताएँ\n",
            "Predicted Translation: प्राथमिकताएँ\n",
            "\n",
            "==================================================\n",
            "\\English sentence: autocomplete\n",
            "Actual Translation: स्वतः समाप्त\n",
            "Predicted Translation: स्वतः समाप्त\n",
            "\n",
            "==================================================\n",
            "\\English sentence: the red joker\n",
            "Actual Translation: लाल जोकर\n",
            "Predicted Translation: लाल जोकर\n",
            "\n",
            "==================================================\n",
            "\\English sentence: symbol browser\n",
            "Actual Translation: अंजूटा ब्राउजर फैक्ट्री\n",
            "Predicted Translation: अंजूटा ब्राउजर फैक्ट्री\n",
            "\n",
            "==================================================\n",
            "\\English sentence: variable list\n",
            "Actual Translation: परिवर्तनीय सूची\n",
            "Predicted Translation: परिवर्तनीय सूची\n",
            "\n",
            "==================================================\n",
            "\\English sentence: keyword\n",
            "Actual Translation: कीवर्ड\n",
            "Predicted Translation: कीवर्ड\n",
            "\n",
            "==================================================\n",
            "Mean Bleu Score = 0.26740518702313065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concat model"
      ],
      "metadata": {
        "id": "zH0d1zmyEacz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the model\n",
        "tf.keras.backend.clear_session()\n",
        "concat_model = encoder_decoder(inp_vocab_size = eng_vocab_size, out_vocab_size = hi_vocab_size,\n",
        "                            embedding_size = 128, enc_lstm_units = 128, dec_lstm_units = 128,\n",
        "                            enc_input_length = enc_input_length, dec_input_length = dec_input_length, \n",
        "                            score_fun = 'concat', att_units = 128)\n",
        "#compiling the model\n",
        "concat_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002), loss = loss_function)\n",
        "#defining the callbacks\n",
        "logdir = '/content/log/concat_model/'\n",
        "# call back of tensorboard\n",
        "tensorboard_callback = TensorBoard(log_dir=logdir,histogram_freq=1, write_graph=True)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "callback = [tensorboard_callback, early_stop]"
      ],
      "metadata": {
        "id": "4yifJppiEXi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "concat_model.fit(train_data_generator, validation_data = val_data_generator, \\\n",
        "              steps_per_epoch = train_eng_inp_seq.shape[0] // batch_size, \\\n",
        "              validation_steps = train_eng_inp_seq.shape[0] // batch_size,epochs = 20, callbacks = callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml98H7rIEefp",
        "outputId": "46225c7e-319d-48f7-b229-8f7985cdddf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "147/147 [==============================] - 21s 122ms/step - loss: 2.2831 - val_loss: 2.1492\n",
            "Epoch 2/20\n",
            "147/147 [==============================] - 17s 115ms/step - loss: 2.0735 - val_loss: 1.9728\n",
            "Epoch 3/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 1.8476 - val_loss: 1.7243\n",
            "Epoch 4/20\n",
            "147/147 [==============================] - 17s 115ms/step - loss: 1.6251 - val_loss: 1.5373\n",
            "Epoch 5/20\n",
            "147/147 [==============================] - 17s 117ms/step - loss: 1.4269 - val_loss: 1.3472\n",
            "Epoch 6/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 1.2279 - val_loss: 1.1601\n",
            "Epoch 7/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 1.0343 - val_loss: 0.9828\n",
            "Epoch 8/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.8520 - val_loss: 0.8269\n",
            "Epoch 9/20\n",
            "147/147 [==============================] - 17s 115ms/step - loss: 0.6915 - val_loss: 0.6890\n",
            "Epoch 10/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.5574 - val_loss: 0.5778\n",
            "Epoch 11/20\n",
            "147/147 [==============================] - 18s 122ms/step - loss: 0.4511 - val_loss: 0.4894\n",
            "Epoch 12/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.3669 - val_loss: 0.4228\n",
            "Epoch 13/20\n",
            "147/147 [==============================] - 17s 115ms/step - loss: 0.3012 - val_loss: 0.3666\n",
            "Epoch 14/20\n",
            "147/147 [==============================] - 19s 132ms/step - loss: 0.2499 - val_loss: 0.3237\n",
            "Epoch 15/20\n",
            "147/147 [==============================] - 17s 117ms/step - loss: 0.2096 - val_loss: 0.2914\n",
            "Epoch 16/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.1781 - val_loss: 0.2662\n",
            "Epoch 17/20\n",
            "147/147 [==============================] - 17s 118ms/step - loss: 0.1530 - val_loss: 0.2445\n",
            "Epoch 18/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.1331 - val_loss: 0.2285\n",
            "Epoch 19/20\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.1178 - val_loss: 0.2164\n",
            "Epoch 20/20\n",
            "147/147 [==============================] - 17s 115ms/step - loss: 0.1044 - val_loss: 0.2045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd30ee1c350>"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "bleu_score = []\n",
        "print(\"=\" * 50)     \n",
        "\n",
        "# sampling 1000 datapoints randomly from test set\n",
        "for index, (_, row) in enumerate(train.sample(1000).iterrows()):\n",
        "    input_sent = row.english\n",
        "    predicted_eng = predict_attention(input_sent, concat_model, plot_attention_weights = False)\n",
        "    actual_eng = row.hi_out.replace('<end>','').strip()\n",
        "    predicted_eng = predicted_eng.replace('<end>','').strip()\n",
        "\n",
        "    # printing Translation Pairs\n",
        "    if (index + 1)%100 == 0:\n",
        "        print(f\"\\English sentence: {input_sent}\")\n",
        "        print(f\"Actual Translation: {actual_eng}\")\n",
        "        print(f\"Predicted Translation: {predicted_eng}\\n\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    bleu_score.append(sentence_bleu([actual_eng.split(),], predicted_eng.split()))\n",
        "\n",
        "print(f\"Mean Bleu Score = {np.mean(bleu_score)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnAVGiZvEmAk",
        "outputId": "ba26e368-1729-42e6-8e4a-bebb26f9ff39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "\\English sentence: y coordinate of the center of rotation\n",
            "Actual Translation: घुमाव केंद्र का निर्देशांक\n",
            "Predicted Translation: घुमाव केंद्र का निर्देशांक\n",
            "\n",
            "==================================================\n",
            "\\English sentence: at the beginning\n",
            "Actual Translation: शुरू में\n",
            "Predicted Translation: शुरू में\n",
            "\n",
            "==================================================\n",
            "\\English sentence: method\n",
            "Actual Translation: विधि\n",
            "Predicted Translation: विधि\n",
            "\n",
            "==================================================\n",
            "\\English sentence: position\n",
            "Actual Translation: स्थिति\n",
            "Predicted Translation: स्थितिः\n",
            "\n",
            "==================================================\n",
            "\\English sentence: sip settings\n",
            "Actual Translation: जमावट\n",
            "Predicted Translation: जमावट\n",
            "\n",
            "==================================================\n",
            "\\English sentence: home phone\n",
            "Actual Translation: फोन निवास\n",
            "Predicted Translation: फोन निवास\n",
            "\n",
            "==================================================\n",
            "\\English sentence: helsinki\n",
            "Actual Translation: हेलसिंकी\n",
            "Predicted Translation: हेलसिंकी\n",
            "\n",
            "==================================================\n",
            "\\English sentence: corrupted files\n",
            "Actual Translation: खराब फ़ाइल\n",
            "Predicted Translation: खराब फ़ाइल\n",
            "\n",
            "==================================================\n",
            "\\English sentence: co mponent\n",
            "Actual Translation: घटक\n",
            "Predicted Translation: घटक\n",
            "\n",
            "==================================================\n",
            "\\English sentence: more children\n",
            "Actual Translation: अधिक संतति\n",
            "Predicted Translation: अधिक संतति\n",
            "\n",
            "==================================================\n",
            "Mean Bleu Score = 0.3123314219212911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "concat_model.save_weights('/content/model_save',save_format='tf')\n",
        "\n",
        "# # Recreate the exact same model purely from the file\n",
        "# new_model = keras.models.load_model('path_to_my_model')\n"
      ],
      "metadata": {
        "id": "llCD8brFExdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive('concat_model', 'zip', '/content/model_save')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "cw5OUUVgLo6D",
        "outputId": "a35330e6-7676-4f84-ebaf-46f40f6911da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/concat_model.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dpJRqQtfMWMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}